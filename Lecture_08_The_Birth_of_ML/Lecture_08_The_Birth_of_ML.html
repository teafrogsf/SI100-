<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>Lecture_08_The_Birth_of_ML</title>
    <link rel="shortcut icon" href="./favicon.ico" />
    <link rel="stylesheet" href="./dist/reset.css" />
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="./dist/theme/simple.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/github.css" />

    <link rel="stylesheet" href="./_assets/assets/custom.css" />
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section data-markdown data-separator="<!--s-->" data-separator-vertical="<!--v-->">
          <textarea data-template>
            


<div style="display: flex; justify-content: center; align-items: center; height: 700px;">
  <div style="text-align: center; padding: 40px; background-color: white; border: 2px solid rgb(0, 63, 163); border-radius: 20px; box-shadow: 0 0 20px rgba(0,0,0,0.1);">
    <h1 style="font-size: 48px; font-weight: bold; margin-bottom: 20px; color: #333;">SI100+ 2024 Lecture 7</h1>
    <p style="font-size: 24px; color: #666;">人类是有极限的！——机器学习的诞生</p>
    <p style="font-size: 16px; color: #999; margin-top: 20px;">SI100+ 2024 Staff | 2024-09-09</p>
  </div>
</div>

<!--s-->

<div class="middle center">
  <div style="width: 100%">

  # Part.0 这是魔法还未诞生的故事...
  
  </div>
</div>

<!--v-->

## 回到回到魔法盒子

<img src="images/magic_box_ml.png" width="85%" style="display: block; margin: 0 auto;">

- 第一个魔法盒子里有一部分输入数据、对应的输出数据 <!-- .element: class="fragment" -->
- 这个魔法盒子根据它们输出模型 <!-- .element: class="fragment" -->
- 第二个魔法盒子根据模型和另一部分输入数据（也就是测试集，test set）输出结果 <!-- .element: class="fragment" -->

<!--v-->

## 魔法？

你可能在各路营销号或有技术的~~营销号~~公众号等自媒体见到过一些AI笑话

- “AI就是魔法” <!-- .element: class="fragment" -->
- “随便动了一个超参数模型就跑起来了” <!-- .element: class="fragment" -->
- ~~我的空间~~ <!-- .element: class="fragment" -->

<img src="images/qzone.png" width="85%" style="display: block; margin: 0 auto;"> <!-- .element: class="fragment" -->

<!--v-->

## 魔法！

然而，在机器学习的数据量还未飞升的年代，理论计算机科学家与数学家曾经也有过理论化的机器学习的美好愿望

- 机器学习理论（machine learning theory）最先研究的对象也是比较传统的机器学习算法
- 我们的第二章也将从这里开始

<!--v-->

## “机器学习”和“人工智能”有什么区别？

- 事实上，这两个名词也经常被混用 <!-- .element: class="fragment" -->
- 很多时候它们会指代相同的意思 <!-- .element: class="fragment" -->
- 不过，如果有人跟你强调TA在讲“机器学习”（比如名叫“机器学习”）的课程，那总体上重点会放在传统机器学习上 <!-- .element: class="fragment" -->


<!--s-->

<div class="middle center">
  <div style="width: 100%">

  # Part.1 感知机，神经网络的起源（不保证100%真的是起源）
  
  </div>
</div>

<!--v-->

## 回忆一下上节课

TODO!

<!--v-->

## 我们已经完全了解机器学习了，能不能来点实战

我们尝试让盒子不再是魔法。

- 上节课举的“学习汉字”的例子中，我们并不知道“纠错”是如何实现的
- 有没有办法在一个例子中具象化纠错的过程？

<!--v-->

## 让我们找一个高中学过的二维平面上的问题

现在你有一个二维平面，平面上有一个点集及其标签$\{(x_1^{(i)},x_2^{(i)},y^{(i)})\}$，其中$x_j^{(i)}\in\mathcal R,y^{(i)}\in\{0, 1\}$。

现在，我们想找到一个向量，能够尽可能地**分割**$y=1$的点（也可以叫它们正样本）和$y=0$的点（也可以叫它们负样本），使得在向量同一侧的点尽可能都是同一类型。

> 这里我们表示点的方式有些特殊。首先，形如$\{x\}$的记号一般用来表示一个集合。我们在高中表示一个点的形式通常是$(x,y)$，但在这里，点的坐标用$(x_1,x_2)$来表示，而$y$指的是标签。在高中，如果我们想表示第$i$个点，通常会把它写成$(x_i,y_i)$的形式，而在这里下标也变成了上标，还给$i$加上了括号。第一次看见它的时候可能会感觉有点奇怪~~好吧看很多次可能都会感觉很奇怪~~，但这样的表示方式是为了未来的方便。（~~不过加括号是为什么我也不知道~~）

<!--v-->

## 让我们找一个高中学过的二维平面上的问题

<img src="1.png" width="40%" style="display: block; margin: 0 auto;"> <!-- .element: class="fragment" -->

大家可以思考一下如何用人类智慧或者算法解决这个问题。<!-- .element: class="fragment" -->

<q>上述提到的点集都是训练集。在实际应用中，你还需要把找到的向量拿到测试集上进行测试，查看你找到的向量分割的效果如何。你发现了没有？其实这里我们隐含了一个假设，就是训练集和测试集是类似的，或者说，**来源于同一分布**。我们将在这节课的扩展部分稍微多聊聊这里的内容。</q> <!-- .element: class="fragment" -->

<!--v-->

## 让我们找一个高中学过的二维平面上的问题

<img src="1.png" width="40%" style="display: block; margin: 0 auto;">

大家可以思考一下如何用人类智慧或者算法解决这个问题。

> 如果你在网上查找感知机有关的内容，你可能会发现它们的文章中提到的任务想找到的是一条直线。请思考：这个任务和我们提到的找向量有区别吗？如果有，是什么区别？

<!--v-->

## 人类智慧

用肉眼观察可得：注意到在上面的例子里有一条向量可以完全分开两个类别，用强大的人类大脑可以把这条直线画出来。

<img src="image-10.png" width="45%" style="display: block; margin: 0 auto;"> <!-- .element: class="fragment" -->

> 如果一个点集真的能被证明可以完全分开成两个类别，我们称这样的点集是线性可分的（linear seperable）。

<!--v-->

## 算法？

欢迎发挥你的脑洞踊跃抢答。如果你想提出传统算法，请用数据一步到位地输出答案。如果你想提出一个机器学习算法，请描述你是如何实现“纠错”的。

想到答案了吗？没有想到也没关系~~想到了你就是先天机器学习圣体~~，我们来看看1957年的人类是怎么做的。

<!--v-->

## 感知机（Perceptron）

感知机由美国学者Frank Rosenblatt在1957年提出。它是怎么处理上面的问题的呢？其实非常简单：

- Step 1: 设我们要找的向量$\boldsymbol{w}=(w_1,w_2)$，其中$w_1,w_2$为参数，它们在最开始会初始化为一个随机值。
  > 这样的随机初始化在机器学习中非常常见。
- Step 2: 对于点$i$，如果$\boldsymbol{w}\cdot\boldsymbol{x^{(i)}}\ge0$但$y^{(i)}<0$，更新$\boldsymbol{w}\leftarrow\boldsymbol{w}-\boldsymbol{x}$；如果$\boldsymbol{w}\cdot\boldsymbol{x^{(i)}}<0$但$y^{(i)}>0$，更新$\boldsymbol{w}\leftarrow\boldsymbol{w}+\boldsymbol{x}$。
- Step 3: 一直重复Step 2，直到无法再更新为止。然后，感知机就会输出$\boldsymbol{w}$，你可以拿着它去测试集看看结果了。

就这么简单！

<!--v-->

## 感知机（Perceptron）

<img src="perceptron_work.gif" width="45%" style="display: block; margin: 0 auto;">

> 其实，找向量和找直线并没有本质上的区别，毕竟找到了向量，你总有办法找到一条截距合适的直线。但是，之所以我们用找向量来描述这个任务，也是因为用向量的形式描述感知机的过程非常简便。感兴趣的同学可以试着用上面的思路解决找直线的问题，然后思考一下你得出的算法会不会有什么奇怪的地方......比如我们刚刚提到的截距。

<!--v-->

## 感知机的设计思路

这是一个非常简单而优雅的算法。TODO 设计思路！

<!--v-->

## 成功抵达终点

不过，你可能会有一个小小的疑问：这个算法真的能找到解而不一直循环吗？

轮到数学家们出手了！

Theorem 1 (Perceptron Convergence Theorem)：如果一个点集是线性可分的，且满足对任意点$i$均有$||\boldsymbol{x}^{(i)}||\le 1$，那么感知机一定可以在不多于$\frac1{\delta^2}$的错误次数内找到合法解$\boldsymbol{w}$。

> 我们将在这节课的扩展部分稍微多聊聊这里的内容。

<!--v-->

## 不过，如果点集不是线性可分的话......

就像这种情况。肉眼观察可得，肯定不存在一条向量能把这些点按正负分成两半。

<img src="images/perceptron_non_lin_sep_ex.png" width="85%" style="display: block; margin: 0 auto;"> <!-- .element: class="fragment" -->

<img src="image-12.png" width="40%" style="display: block; margin: 0 auto;">

在这种情况下，我们的感知机会不幸死机。

但是，没有人规定机器学习一定要做到100%准确率，哪怕是在训练集上。因此，只要我们保证我们的算法能跑完就好了，至少这样我们能跑出一个结果给我们的甲方爸爸看。换句话说，我们希望我们的机器学习算法能够**收敛（converge）**到一个结果上。

怎么办呢？

<!--v-->

<!--v-->

## 听这课还摸鱼的人有难了

- 众所周知，在我们日常的学习过程中，不是老师教了什么我们就学会了什么。我们吸收知识的进度和老师的实际教学进度是有差距的。
- 更众所周知的是，如果一堂课上的太久，我们学习的效率是要下降的。如果拖得太久，那我们上课可能就会掉线~~开摆~~，完全听不进去了。

但机器不是这样。只要你不停止程序运行，它可以一直学习下去。因此，我们希望机器的学习过程能够慢慢停止到一个合适的结果上。

<!--v-->

## 听起来模仿人类很合理，但总感觉有什么不对劲

可是机器为什么要这么做？人类学不了太久是因为学不动了，但机器明明可以一直学啊？

......咦，一直学就是最优的吗？

<!--v-->

## 学到最高点

从上面线性不可分的例子就可以看出，让机器一直学习并不一定会达到最优解。

事实上，从更一般的角度去看，我们可以把参数看成自变量，学习效果看成因变量（也就是把它们看成函数），于是机器学习就是通过改变自变量来尝试找到这个函数的最优（optimum）。

但是，假如一个机器学习算法每一步令参数变化的量都一直保持不变的话，那么就有可能出现在最优点附近反复横跳的情况。

<img src="images/no_optimum.png" width="85%" style="display: block; margin: 0 auto;"> <!-- .element: class="fragment" -->

<!--v-->

## 学习率（learning rate）和学习率衰减（learning rate decay）

因此，我们需要给机器学习算法设置一个**学习率**，使其每一步更新参数都受到学习率的约束。同时，这个学习率还有必要逐步降低，从而让机器学习算法逐步走向收敛。

<img src="images/optimum.png" width="85%" style="display: block; margin: 0 auto;"> <!-- .element: class="fragment" -->

学习率和学习率衰减与机器学习算法的关系是什么？
- 它们不会被机器学习算法主动更新，却影响着学习的结果
- 类似这样的参数叫做超参数（hyperparameter）

> 当然，我们收敛的位置也不一定是，或者说，大概率不是完美的最优点。这部分的内容将会在后续进行更多探讨。

<!--v-->

## 改进后的感知机

- Step 1: 设我们要找的向量$\boldsymbol{w}=(w_1,w_2)$，其中$w_1,w_2$为参数，它们在最开始会初始化为一个随机值。初始化一个学习率$\eta$和学习率衰减函数$f(\eta)$。
- Step 2: 对于点$i$，如果$\boldsymbol{w}\cdot\boldsymbol{x^{(i)}}\ge0$但$y^{(i)}<0$，更新$\boldsymbol{w}\leftarrow\boldsymbol{w}-\eta\boldsymbol{x}$；如果$\boldsymbol{w}\cdot\boldsymbol{x^{(i)}}<0$但$y^{(i)}>0$，更新$\boldsymbol{w}\leftarrow\boldsymbol{w}+\eta\boldsymbol{x}$。
- Step 3: 令$\eta\leftarrow f(\eta)$，一直重复Step 2，直到无法再更新或者更新效果低于某个阈值为止。然后，感知机就会输出$\boldsymbol{w}$，你可以拿着它去测试集看看结果了。

> 其实在现在实际的机器学习训练中，$\eta$的取值可能远比你之前想象的要小——$\eta=0.001$甚至$0.0001$都是很有可能的。而$f(\eta)$也是一个有很多选择的东东。一个简单的例子就是$f(\eta)=\alpha\eta$，其中$\alpha$是个很接近但小于1的数比如$0.999$。

<!--s-->

<div class="middle center">
  <div style="width: 100%">

  # Part.2 再一次认识机器学习
  
  </div>
</div>

<!--v-->

## 回顾上述的一整个流程

实际上，之前讲过的流程已经基本体现了一个机器学习算法的所有行为。现在我们把它们抽象一下，就得到了一个机器学习算法的完整流程（这个流程会比上节课更加正式）：

- Step 1: 随机初始化参数；
- Step 2: 给模型输入训练集的数据，得到模型的输出，也就是**预测（prediction）**
- Step 3: 计算预测与真实结果的差距，也就是**损失（loss）**
- Step 4: 根据损失优化原来的参数
- Step 5: 调整部分超参数，重新回到Step 2，直到满足算法的终止条件

......嗯？等等，你这给我干哪来了？

<!--v-->

## 嗯？嗯？嗯？

上面说的流程似乎和之前说的感知机的流程有那么亿点点区别。

- 感知机的纠错是实时的，输入一个数据就可以纠错一次，而不是一股脑输入全部的数据
- 感知机中预测与真实结果的差距就是一个条件判断，损失是什么？优化和损失有关系吗？
- 如果你在网上查找感知机，可能会发现对它的描述有不同版本；在有的版本里，它们的感知机也是一次性输入全部数据的
- 你可能还会发现它们描述优化参数的过程非常复杂，一点也没有我们上述提到的算法简约
- 这其中有什么区别~~本手妙手俗手~~吗？

<!--v-->

## 嗯？嗯？嗯？

TODO!

> 这里涉及到了机器学习实战中epoch和batch的概念


<!--s-->

<div class="middle center">
  <div style="width: 100%">

  # Part.3 更多的任务，更多的定义
  
  </div>
</div>

<!--v-->

## 人总是离不开分类学（双关）的

我们已经意识到，机器可以帮我们解决各种各样的问题。作为人类，我们能帮机器做的就是把这些问题，或者说任务分好类，从而面对不同的任务挑出合适的机器来解决。~~或者面对不同的任务掏出相同的GPT~~

感知机用它强大的智慧把一个点集分为了两部分。类似这样把一个点集分成若干个部分的任务，我们是不是可以给它们统一取个名字？

- **分类（classification）**
- 具体来说，感知机解决的是**二分类（binary classification）**问题
- 分类是机器学习的一大核心任务

<!--v-->

## 分类是个很大的领域

其实，很多问题的背后都是分类问题。

- 人脸识别你是班里的哪一个人是分类（我们一般把这种分类叫做多类别分类（multi-class classification））
- 把一个人贴很多社交标签也是分类（我们一般把这种分类叫做多标签分类（multi-label classification））
- ...... TODO

分类不能解决什么样的问题？

<!--v-->

## 如果问题的要求是尽可能地逼近真实值......

让我们再次翻开高中课本，找到我们高中学过的（二维平面上的）最小二乘法（希望大家都学过）。

其目标是寻找一个函数，使得所有观测点到该函数的距离的平方和最小，从而在给定横坐标的情况下预测对应的纵坐标

<div align=center>
<img src="image-3.png" width=400> 
<div align=left> 

<!--v-->

## 如果问题的要求是尽可能地逼近真实值......

这个任务为什么不能被分类概括？它和分类任务有什么区别？

像这样的，目标是让预测结果尽可能接近真实结果的任务，叫做**回归（regression）**。

<!--v-->

## 分类与回归，离散与连续

如何区分分类和回归呢？

从人话的角度，其实分类也是在*接近*真实结果对吧？

好像也有那么一点不对——分类希望的是和真实结果*完全一致*——预测和结果是同一个类别。

找到概括的方法了吗？

* 事实上，一切机器学习的任务都可以分成**分类**和**回归**

* **分类**就是对数据分进行分类（好吧好像是废话~），预测的结果往往是一系列**离散**的数据
* **回归**是一种数学模型，利用数据统计原理，对大量统计数据进行数学处理，建立一个相关性较好的回归方程（函数/映射），预测的事实上是一个**连续**的结果

<!--v-->

## 


上面说的不对，分类和回归的唯一区别在于损失函数不同

<!--v-->

## 其他的传统机器学习算法

TODO!

## 再次再次回到魔法盒子

<img src="images/magic_box_ml.png" width="85%" style="display: block; margin: 0 auto;">

如果，我们的机器学习算法不需要ground-truth？

<!--v-->

## 疑似逻辑陷阱

<!--v-->

## 经典的聚类算法——KNN (K-Nearest Neighbors)

* 顾名思义，就是k个最近的邻居（k近邻）
* KNN的原理就是**当预测一个新样本的类别时，根据它距离最近的 K 个样本点是什么类别来判断该新样本属于哪个类别（多数投票）**

* **距离度量、k值的选择及分类决策规则**是k近邻法的三个基本要素

<!--v-->

## 经典的聚类算法——KNN (K-Nearest Neighbors)

<div align=center>
<img src="image-4.png" width=700> 
<div align=left> 

* 图中绿色的点就是我们要预测的那个点，假设K=3。那么KNN算法就会找到与它距离最近的三个点（这里用圆圈把它圈起来了），看看哪种类别多一些，比如这个例子中是蓝色三角形多一些，新来的绿色点就归类到蓝三角了

* **距离度量、k值的选择及分类决策规则**是k近邻法的三个基本要素

<!--v-->

## 机器学习的学习方式

### 监督学习

* 在KNN中我们会发现：初始的点是有颜色的，所有的data本质都是一个点以及其颜色的二元组，也就说不同的x对应不同坐标的点，不同的y对应点的颜色，即

$$ (点的坐标，颜色) $$

* 像这种提供**输入数据**（比如KNN里的点坐标）和其对应的**标签数据（label**，KNN的点的颜色），然后搭建一个模型，模型经过训练后准确的找到输入数据和标签数据之间的**最优映射关系**，从而对新的未标记数据进行预测或分类，我们称作**监督学习(supervised learning)**

<!--v-->

## 监督学习

* 没听懂？我们再举一个例子
* 假如有一群 草泥马 和 牛马 组成的马群，这时候需要一个机器对马群进行分类，但是这个机器不知道 草泥马 和 牛马 长什么样儿，所以我们首先拿一堆 草泥马 和 牛马 的照片给机器看，告诉机器 草泥马 和 牛马 长什么样儿。机器经过反复的看，形成肌肉记忆，可以对 草泥马 和 牛马 形成自己的定义，然后机器就可以准确的对马群进行分类。
<div align="center">
    <img src="image-5.png" alt="图片1" width="300"height="310">
    <img src="image-6.png" alt="图片2" width="300">
</div>

<!--v-->

## 无监督学习（unsupervised learning）

* 定义:**训练数据只包含输入样本，没有相应的标签或目标**

* 无监督学习目标不是告诉计算机怎么做（没有label），而是让它自己去学习怎样做事情

* 包装一下：我们只是单纯的将这两种马的照片给机器看，并没有告诉机器哪些是牛马，哪些是草泥马，机器在观察图片后发现马有两类，并能分辨出草原上的马分别属于哪一类

<!--v-->

## 无监督学习（unsupervised learning）

### 你是否在想，监督学习是因为找到了映射关系才能对图片进行分类的

### 那无监督学习凭啥能把相似的图片认出来？

* 无监督学习会从无标签数据中学习有效的特征或表示，而同一类的图片具有相似的特征
* 特征提取（Feature Extraction）：将任意数据（如文本或图像）转换成机器学习的数学特征

<!--v-->

## 分类与回归的再分类

* 通过上面的描述，聪明的你肯定已经发现**回归任务本质上就是一种监督学习**，因为回归任务实际上是在是在数据和标签的基础上学习一个最优的函数曲线

* 而分类任务则相对复杂，可以细分为 **分类(classification)** 和 **聚类(clustering)**两种

* 有点懵是吧？正常。请注意前面的分类任务四字中的**分类**只是泛指，后面的分类则是专指**已知数据和其标签来进一步训练模型进行分类任务**

* 换言之，
**分类**是一种**监督学习**，例如告诉机器哪张是牛马哪张是草泥马
而**聚类**是一种**无监督学习**，例如之前提到的KNN算法

* 现在也有很多文章直接把机器学习的任务直接分为**分类**，**聚类**，**回归**三类，是更好的选择

<!--v-->

## 分类与回归的再分类

<img src="image-8.png" width="85%" style="display: block; margin: 0 auto;">

<!--v-->

## 认识到优质数据的稀缺性

* 在信息化社会，数据被誉为新的石油然而，与之相反的是，我们却面临着优质数据的严重缺乏。这种现象引发了一系列的问题，特别是在人工智能（AI）和机器学习（ML）领域，这一问题尤为突出。

* 什么是优质数据？

* 优质数据是指具有高度准确性、一致性、完整性和可靠性的数据。这种数据不仅要精确无误，还要对我们要解决的问题有直接的相关性。

* 还有一些领域，比如医疗领域，获取足够的数据本身就是一个挑战。

<!--v-->

## 半监督学习（Semi-supervised learning）（仅做了解）

* 顾名思义，半监督学习利用**少量有标签的数据和大量无标签**的数据来训练模型

* 在数据稀缺条件下的被迫选择

<!--s-->

<div class="middle center">
  <div style="width: 100%">

  # Part.3 机器学习的分类
  
  </div>
</div>

<!--v-->

<img src="image-7.png" width="85%" style="display: block; margin: 0 auto;">

<!--v-->

<img src="image-8.png" width="85%" style="display: block; margin: 0 auto;">

<!--s-->

<div class="middle center">
  <div style="width: 100%">

  # Part.4 总结，与......
  
  </div>
</div>

<!--v-->

## 总结

我们今天讲了什么？ <!-- .element: class="fragment" -->

- 一个故意挖坑的引子 <!-- .element: class="fragment" -->
- 一个课程大纲的详细版 <!-- .element: class="fragment" -->
- 算法的基本概念 <!-- .element: class="fragment" -->
- 从传统算法走向机器学习 <!-- .element: class="fragment" -->
- <span> ~~下下集预告~~ </span> <!-- .element: class="fragment" -->

......如果你还愿意听下去的话，那么，接下来是时候来点猛料大杯大脑升级了。 <!-- .element: class="fragment" -->

## 回到感知机（optional）
          </textarea>
        </section>
      </div>
    </div>

    <script src="./dist/reveal.js"></script>

    <script src="./mermaid/dist/mermaid.min.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        slideNumber: true,
        highlight: {
          highlightOnLoad: false
        },
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"transition":"slide","transitionSpeed":"fast","center":false,"slideNumber":"c/t","width":1000,"_":["./Lecture_08_the_birth_of_ML/Lecture_08_The_Birth_of_ML.md"],"static":"static/Lecture_08_The_Birth_of_ML"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
      Reveal.addEventListener('ready', function (event) {
        const blocks = Reveal.getRevealElement().querySelectorAll('pre code:not(.mermaid)');
        const hlp = Reveal.getPlugin('highlight');
        blocks.forEach(hlp.highlightBlock);
      });
    </script>

    <script>
      const mermaidOptions = extend({ startOnLoad: false }, {});
      mermaid.startOnLoad = false;
      mermaid.initialize(mermaidOptions);
      const cb = function (event) {
        mermaid.init(mermaidOptions, '.stack.present > .present pre code.mermaid');
        mermaid.init(mermaidOptions, '.slides > .present:not(.stack) pre code.mermaid');
      }
      Reveal.addEventListener('ready', cb);
      Reveal.addEventListener('slidetransitionend', cb);
    </script>
  </body>
</html>
