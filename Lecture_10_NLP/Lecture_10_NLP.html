<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

  <title>Lecture_10_NLP</title>
  <link rel="shortcut icon" href="./favicon.ico" />
  <link rel="stylesheet" href="./dist/reset.css" />
  <link rel="stylesheet" href="./dist/reveal.css" />
  <link rel="stylesheet" href="./dist/theme/simple.css" id="theme" />
  <link rel="stylesheet" href="./css/highlight/github.css" />

  <link rel="stylesheet" href="./_assets/assets/custom.css" />
</head>

<body>
  <div class="reveal">
    <div class="slides">
      <section data-markdown data-separator="<!--s-->" data-separator-vertical="<!--v-->">
        <textarea data-template>
            
<p style="font-size: 16px; color: #999; margin:5px; position: absolute;"><a href="..">Homepage</a> | <a href="?print-pdf">Printable Version</a></p>
<div style="display: flex; justify-content: center; align-items: center; height: 700px;">
  <div style="text-align: center; padding: 40px; background-color: white; border: 2px solid rgb(0, 63, 163); border-radius: 20px; box-shadow: 0 0 20px rgba(0,0,0,0.1);">
    <h1 style="font-size: 48px; font-weight: bold; margin-bottom: 20px; color: #333;">SI100+ 2024 Lecture 10</h1>
    <p style="font-size: 24px; color: #666;">用文字回应你的期待——自然语言处理</p>
    <p style="font-size: 16px; color: #999; margin-top: 20px; margin-bottom:5px">SI100+ 2024 Staff | 2024-09-13</p>
  </div>
</div>

<!--s-->



<div class="middle center"><div style="width: 100%">

# Intro: ChatGPT 的诞生

</div></div>

<!--v-->

## ChatGPT

- <span> ChatGPT: Chat Generative Pre-trained Transformer </span> <!-- .element: class="fragment" -->
- <span> 2022 年 11 月 30 日，ChatGPT 横空出世，短短5天，注册用户数就超过100万。 </span> <!-- .element: class="fragment" -->
- <span> 2023 年一月末，ChatGPT的月活用户已突破1亿，成为史上增长最快的消费者应用。 </span> <!-- .element: class="fragment" -->
- <span> 笔者当时拿它做疫情期间的英语试卷，比我分还高 /TwT/ </span> <!-- .element: class="fragment" -->
- <span> 引发了一火车 **新闻学乱象** </span> <!-- .element: class="fragment" -->

![img|900](image.png)


<!--s-->

<div class="middle center"><div style="width: 100%">

# AI 的两大流派 - NLP VS CV

</div></div>

<!--v-->

## 自然语言处理（NLP, Natural Language Processing）

- <span> 顾名思义：关注如何让计算机理解、解释和生成人类语言。 </span> <!-- .element: class="fragment" -->
- <span> 早在人工智能兴起之前，NLP就是计算机科学的重要研究方向之一。我们很多熟知的技术，如搜索引擎、机器翻译、语音识别、人声合成等，都是NLP的应用。 </span> <!-- .element: class="fragment" -->
- <span> NLP的目标是让计算机能够像人类一样理解和处理自然语言，从而实现更智能的人机交互。 </span> <!-- .element: class="fragment" -->
- <span> 在 ChatGPT 出现以前，人们通过特定的基于规则的算法来实现对文本的理解。例如，在中英文翻译中，人们会编写大量的规则来处理不同的语法结构和词汇，通过将句子拆分成词语、短语等基本单元，然后逐步翻译。 </span> <!-- .element: class="fragment" -->

<!-- FIXME: 

Reference: 
Google
from
PBMT(phrase-based machine translation) 
to
GNMT(Google Neural Machine Translation)
-->

- <span> 很多翻译软件中也自带了朗读功能，这就是语音合成技术的应用。对于语音合成，一个简单直接的方法是将文本逐字（单词）地拆分，然后通过查表的方式找到对应的音素（音位）或音频片段，最后将这些音素或音频片段拼接起来，形成连贯的语音。但这种方法的效果并不好，因为不同的语气（例如疑问句、陈述句）和语境会导致句子中的音调、语速等发生变化，而这些变化是通过简单的查表方式无法捕捉到的。 汉字存在的多音字，也让规则变得极为复杂。 </span> <!-- .element: class="fragment" -->

<!--v-->

## 计算机视觉（CV, Computer Vision）

- <span> 让计算机能够像人类一样理解和解析视觉世界。 </span> <!-- .element: class="fragment" -->
- <span> NLP 聚焦的是人类语言，而 CV 则是人类视觉。这两个领域都是 AI 的重要分支，他们在面对的任务和处理方式迥然不同，但在发展的过程之中又相辅相成。 </span> <!-- .element: class="fragment" -->
- <span> 日常生活中，上班刷脸打卡、美颜相机、体感游戏、自动驾驶等等，都在应用计算机视觉技术。 </span> <!-- .element: class="fragment" -->
- <span> 这些应用就对应了 CV 中的几个主要任务 </span> <!-- .element: class="fragment" -->
    - <span> 图像分类（Image Classification）： 识别图像所属的类别。 </span> <!-- .element: class="fragment" -->
    - <span> 人脸识别（Face Recognition）： 识别并确认图像或视频中的人脸。 </span> <!-- .element: class="fragment" -->
    - <span> 目标检测/追踪（Object Detection&Tracking）： 找出图像中的目标及其位置。 </span> <!-- .element: class="fragment" -->
    - <span> 语义分割（Semantic Segmentation）： 将图像划分为具有不同语义的区域。 </span> <!-- .element: class="fragment" -->

<!-- ## 其他流派

- 语音识别: 将口语转换为书面文本
    - 任务: 语音转文本、语音分析、语音生成
    - 应用: 语音助手、自动翻译电话、无障碍沟通技术
- 强化学习: 关注如何通过奖励和惩罚机制指导代理（agent）学习策略。
    - 任务: 策略优化、环境交互、奖励机制设计
    - 应用: 游戏AI、机器人控制、自动化交易系统
- 生成对抗网络（Generative Adversarial Networks, GANs）: 由生成器和判别器组成的模型框架，用于生成接近真实数据的假数据。
    - 任务： 图像生成、图像超分辨率、数据增强。
    - <span> 应用： 图像合成、虚拟现实、艺术创作。 --> </span> <!-- .element: class="fragment" -->

<!--v-->

## CV 与 NLP 的早期发展

- <span> 计算机视觉的发展在深度学习时代率先取得了显著突破，其关键事件是上节课说的 AlexNet。 </span> <!-- .element: class="fragment" -->
    - <span> AlexNet 使用 CNN 取得了远超传统方法的准确率，激发了研究热潮。 </span> <!-- .element: class="fragment" -->
    - <span> 随后几年，计算机视觉领域涌现出一系列创新，包括VGGNet、GoogLeNet、ResNet等，更深更复杂的网络模型不断刷新图像识别的性能记录。 </span> <!-- .element: class="fragment" -->
- <span> 与此同时，自然语言处理（NLP）主要依赖于循环神经网络（RNN）及其变种，如LSTM和GRU，来处理序列数据。 </span> <!-- .element: class="fragment" -->
    - <span> 然而，RNN结构在处理长距离依赖和并行计算方面存在固有的局限性。 </span> <!-- .element: class="fragment" -->
    - <span> 2017年，Vaswani等人在论文 [**《Attention is All You Need》**](https://arxiv.org/abs/1706.03762) 中提出了Transformer模型，彻底改变了NLP领域。 </span> <!-- .element: class="fragment" -->
        - <span> 之前了解过 AI 的同学对这个文章名应该不陌生 ~~Money is All You Need~~ </span> <!-- .element: class="fragment" -->
    - <span> Transformer摒弃了RNN结构，采用了自注意力机制（self-attention mechanism），极大地改善了处理长距离依赖和并行计算的能力。 </span> <!-- .element: class="fragment" -->

<!--v-->

## NLP 的异军突起

<div class="row">

<div class="col">

- <span> 基于Transformer的 **语言模型** 如BERT、GPT系列、T5等，迅速成为NLP的主流技术。 </span> <!-- .element: class="fragment" -->
- <span> 这些模型大幅提升了各类NLP任务的表现，从文本分类、机器翻译，到问答系统和文本生成。 </span> <!-- .element: class="fragment" -->
- <span> 更可怕的是…… </span> <!-- .element: class="fragment" -->
- <span> Transformer不止局限于NLP，也开始在**计算机视觉、语音处理等**领域展现出强大的通用性和适用性。 </span> <!-- .element: class="fragment" -->

</div>

<div class="col">

![蛙蛙抱怨|380](image-1.png) <!-- .element: class="fragment" -->

</div>
</div>

<!--s-->

<div class="middle center"><div style="width: 100%">

# 这么厉害？那 NLP 到底在干什么？

</div></div>

<!--v-->

## NLP 研究的思维方式

- <span> 传统的NLP研究主要依赖于 </span> <!-- .element: class="fragment" -->
    - <span> 规则：专家手动编写语言规则，例如语法规则和词典，这些用于进行各种语言处理任务。 </span> <!-- .element: class="fragment" -->
    - <span> 统计方法：基于大规模的语言数据，利用统计学原理，构建如隐马尔可夫模型（HMM）等进行语言建模。 </span> <!-- .element: class="fragment" -->
- <span> 在引入人工智能之后，自然语言处理领域产生了不同的方法学流派： </span> <!-- .element: class="fragment" -->
    - <span> 符号主义（Symbolism）：主张认知的本质可以看作是符号的操纵，关注显式规则和逻辑推理，强调知识的表达和推理，比如基于知识图谱的方法。 </span> <!-- .element: class="fragment" -->
    - <span> 连接主义（Connectionism）：即神经网络方法，主张认知过程是大量简单单元（神经元）之间相互连接和相互作用的结果，强调通过大量数据和神经网络模型进行学习，典型代表如深度学习和word2vec。 </span> <!-- .element: class="fragment" -->

<!--v-->

## NLP 的研究

- <span> 模型研究（基础任务研究）： </span> <!-- .element: class="fragment" -->
    - <span> 这类研究主要关注开发新的算法或模型，目标是提升对自然语言理解的基本能力。比如 Attention 机制、AlexNet，RNN 等，都是提出新的网络结构，从而给 NLP 领域解锁了新的可能性。每一个新的模型都相当于一个新的积木块，可以用来构建更复杂的系统。 </span> <!-- .element: class="fragment" -->
- <span> 下游任务研究： </span> <!-- .element: class="fragment" -->
    - <span> 这部分研究关注如何将已有的模型应用于特定的实际任务，如机器翻译、文本生成等。它们相当于利用基础研究中创造的积木块，来构建复杂的系统或解决具体的问题。 </span> <!-- .element: class="fragment" -->
<!--v-->

## 一些数学...

- <span> 在理解现代NLP（尤其是神经网络方法）时，线性代数是一个重要的工具，因为想要借助神经网络或者概率模型来处理自然语言，就必须要把一个一个的文字转化成可以被计算的数字。就像我们可以用 1 到 26 来表示字母，**将文字转化为数字**，就是 NLP 的第一步。 </span> <!-- .element: class="fragment" -->
- <span> 但是，这样 1 到 26 的表示方法是不够的，因为 **我们的文字是有意义的**，而字母只是一种符号，即使是现在最庞大的神经网络，也难以用这样的符号来学习到文字的意义。所以，我们需要一种更加有语意的表示方法，这就是 **向量** </span> <!-- .element: class="fragment" -->

<!--v-->

## 在数学中蕴含语意

- <span> 怎样在数学中蕴含语意呢？ </span> <!-- .element: class="fragment" -->
- <span> 我们中学时都学过向量（Vector）的概念，向量是一个有方向和大小的量，可以用来表示空间中的一个点。 </span> <!-- .element: class="fragment" -->
- <span> 如果我们把每一个单词视作为一个点，把单词投射到一个二维平面。并且我们规定，**语义上相近的单词在这个平面上的距离也要相近**。这样，我们就可以用一个向量来表示一个单词，这个向量就是这个单词的 **词向量（Word Embedding）**。 </span> <!-- .element: class="fragment" -->
- <span> 这样的表示方法，就使得点的坐标也带有了意义。 </span> <!-- .element: class="fragment" -->

<!--v-->

## 举个例子

![坐标|300](image-2.png)

- <span> 把食物的词汇投射到二维平面，并且我们规定越往X轴正方向的单词越甜，越往X轴负方向的单词越苦；越往Y轴正方向的单词越硬，越往Y轴负方向的单词越软。 </span> <!-- .element: class="fragment" -->
- <span> 那么，我们就可以某个第一象限的点表示“苹果”，某个第二象限的点表示“苦瓜”，某个第三象限的点表示“可可”，某个第四象限的点表示“蛋糕”。并且根据定义，我们知道“苹果”和“梨子”在这个平面上的距离将要比“苹果”和“苦瓜”要近。 </span> <!-- .element: class="fragment" -->


<!--v-->

## 举个例子 (cont'd)

![坐标|200](image-2.png)

- <span> 我们先不要管我们是如何将单词投射到这个平面上的（马上会讲到），我们先来关注这样的表示方法有哪些奇妙的性质。 </span> <!-- .element: class="fragment" -->
- <span> 首先，我们可以看到，意思相近的单词在这个平面上的距离也是相近的 </span> <!-- .element: class="fragment" -->
    - <span> 那么机器在学习这样的表示方法时，就可以通过这个 **距离** 来判断两个单词的 **语义相似度**，从而更好地理解自然语言。 </span> <!-- .element: class="fragment" -->
- <span> 其次，我们可以注意到，**坐标轴也是有语意的**！ </span> <!-- .element: class="fragment" -->
    - <span> 也就是说，二维空间上的两根坐标轴分别表示了甜的程度和硬的程度，而如果将这个空间扩展到三维，我们就可以新增一个维度，比如表示酸的程度，这样我们就有了更加丰富的语意表示。依此类推，我们可以将这个空间 **扩展到一个很高的纬度**，从而将丰富的语意信息编码到这个向量中。 </span> <!-- .element: class="fragment" -->

<!--v-->

## 举个例子 (cont'd)

- <span> 那么如果我们能够通过某种算法，让机器 **自动地将单词投射到一个高维的空间中**，使得这个空间中的点的坐标带有语意，那么我们就可以用这个向量来表示单词，这就是词向量的基本思想。 </span> <!-- .element: class="fragment" -->
- <span> 那么，我们是如何将单词投射到这个平面上的呢？这就是我们要讲的词向量的训练方法，其中最著名的就是 word2vec </span> <!-- .element: class="fragment" -->

<!--v-->

## word2vec

- <span> word2vec 是 Google 在 2013 年提出的一种词向量训练方法，它的核心思想是 **通过神经网络来学习单词的词向量**。 </span> <!-- .element: class="fragment" -->
- <span> 它基于一个很朴素的想法：在文本中，**越是经常出现在一起的单词，它们的语义越相近**。 </span> <!-- .element: class="fragment" -->
- <span> 比如说，句子中有“小猪爱吃苹果和梨子”，那么我们就知道“小猪”、“苹果”和“梨子”存在某种联系，比如“小猪”和“苹果”可能是主语和宾语的关系，而“苹果”和“梨子”可能是同义词。那么，我们就可以通过这种关系来学习单词的词向量。 </span> <!-- .element: class="fragment" -->
- <span> word2vec 首先收集了大量的文本数据，然后将句子中的任意两个单词两两组合，作为训练样本。 </span> <!-- .element: class="fragment" -->
- <span> 对于每一个训练样本，word2vec 会通过神经网络来预测这两个单词之间的关系，具体来说，就是预测这两个单词之间的距离。 </span> <!-- .element: class="fragment" -->
    - <span> 如果两个单词在文本中经常出现在一起，那么它们之间的距离就会很小；反之，如果两个单词很少在一起出现，那么它们之间的距离就会很大。 </span> <!-- .element: class="fragment" -->
- <span> 通过这种方式，word2vec 就可以学习到单词的词向量。 </span> <!-- .element: class="fragment" -->

<!--s-->

<div class="middle center"><div style="width: 100%">

# 连接主义

</div></div>

<!--v-->

## 以前的连接主义

- <span> 主要依赖于递归神经网络（RNN）及其扩展 </span> <!-- .element: class="fragment" -->
- <span> RNN可以处理任意长度的序列，适用于各种自然语言任务，如句子分类、机器翻译等。 </span> <!-- .element: class="fragment" -->
- <span> 局限： </span> <!-- .element: class="fragment" -->
    - <span> 梯度消失和爆炸：随着序列长度的增加，梯度在反向传播过程中可能会逐渐消失（变得非常小）或爆炸（变得非常大），导致训练困难。 </span> <!-- .element: class="fragment" -->
    - <span> 长距离依赖问题：RNN在捕捉远距离信息时表现不佳，容易“遗忘”早期重要的信息。 </span> <!-- .element: class="fragment" -->
- <span> 如何解决？ </span> <!-- .element: class="fragment" -->
    - <span> 长短期记忆（LSTM）：通过引入门控机制（输入门、忘记门和输出门）来控制信息的流动，从而有效缓解梯度消失问题。 </span> <!-- .element: class="fragment" -->
    - <span> 门控循环单元（GRU）：是LSTM的简化版，也采用门控机制，但参数更少，使得计算更为高效。 </span> <!-- .element: class="fragment" -->
    - <span> **Attention 机制** </span> <!-- .element: class="fragment" -->

<!--v-->

## Attention 机制

- <span> Attention机制是一种用于增强神经网络捕捉全局信息的技术。其核心思想是：在处理序列的每一步时，不仅关注当前输入，还要“关注”序列中其他相关位置的信息，分配不同的权重给这些位置，从而得到一个更为动态和灵活的信息表示。 </span> <!-- .element: class="fragment" -->
- <span> 在理解Attention机制时，线性代数起到了重要的作用： </span> <!-- .element: class="fragment" -->
    - <span> 向量和矩阵运算：Attention机制通过计算一系列向量和矩阵的点积、加权求和等操作来实现。 </span> <!-- .element: class="fragment" -->
    - <span> Softmax函数：用于将计算出的权重标准化为概率分布。 </span> <!-- .element: class="fragment" -->

<!--v-->

## Attention 的诞生

- <span> Attention机制首先在机器翻译任务中得到应用，可以显著提升翻译质量。最早的Attention模型由Bahdanau等人提出（2014），它引入了一个可学习的权重矩阵来在翻译过程中动态分配注意力，从而更好地捕捉源语言和目标语言之间的对齐关系。 </span> <!-- .element: class="fragment" -->

<!--v-->

## Attention Is All You Need（Attention机制的成功）

- <span> 在2017年，Vaswani等人提出了Transformer模型，彻底改变了NLP领域。这篇论文题为“Attention Is All You Need”，核心观点是：通过Attention机制完全替代传统的RNN/LSTM结构，可以大幅提升模型的并行计算能力和长距离依赖处理能力。 </span> <!-- .element: class="fragment" -->
- <span> Transformer模型的特点 </span> <!-- .element: class="fragment" -->
    - <span> 自注意力机制（Self-Attention）：允许在每个时间步通过Attention机制聚合整个序列的信息。 </span> <!-- .element: class="fragment" -->
    - <span> 多头注意力（Multi-Head Attention）：通过多个独立的Attention头来捕捉不同的子空间表示，从而获得更丰富的信息。 </span> <!-- .element: class="fragment" -->
    - <span> 层归一化和前馈网络：结合线性代数和非线性变换，增强了模型的表达能力与训练稳定性。 </span> <!-- .element: class="fragment" -->

<!--v-->

## CV与NLP的相辅相成

- <span> 在计算机视觉（CV）领域，Attention机制也同样得到了广泛应用。例如，Vision Transformer（ViT）便是将Transformer架构从NLP迁移到CV领域的成功案例。这表明，CV和NLP之间可以相互借鉴和影响，促进彼此的发展。例如： </span> <!-- .element: class="fragment" -->
    - <span> Feature Extraction：在图像处理中，Attention可以帮助提取更重要的特征。 </span> <!-- .element: class="fragment" -->
    - <span> 结构上的共性：CV中的图像信息与NLP中的文本信息都可以被视为高维序列，通过Attention机制处理这些序列能取得显著效果。 </span> <!-- .element: class="fragment" -->
    - <span> 工具和方法的共用：如线性代数、矩阵运算等，对理解和实施Attention机制至关重要，跨越CV和NLP领域。 </span> <!-- .element: class="fragment" -->

<!--s-->

<div style="display: flex; justify-content: center; align-items: center; height: 700px;   ">
  <div style="text-align: center; padding: 40px; background-color: white; border-radius: 20px; box-shadow: 0 0 20px rgba(0,0,0,0.1);">
    <div style="display: inline-block; padding: 20px 40px; border-radius: 10 px; margin-bottom: 20px;">
      <h1 style="font-size: 48px; font-weight: bold; margin: 0; color: rgb(16, 33, 89)">Thanks for Listening</h1>
    </div>
    <p style="font-size: 24px; color: #666; margin: 0;">Any questions?</p>
  </div>
</div>
          </textarea>
      </section>
    </div>
  </div>

  <script src="./dist/reveal.js"></script>

  <script src="./mermaid/dist/mermaid.min.js"></script>

  <script src="./plugin/markdown/markdown.js"></script>
  <script src="./plugin/highlight/highlight.js"></script>
  <script src="./plugin/zoom/zoom.js"></script>
  <script src="./plugin/notes/notes.js"></script>
  <script src="./plugin/math/math.js"></script>
  <!-- Font awesome is required for the chalkboard plugin -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/js/all.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <!-- Custom controls plugin is used to for opening and closing annotation modes. -->
  <script src="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/customcontrols/plugin.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/customcontrols/style.css">
  <!-- Chalkboard plugin -->
  <script src="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/chalkboard/plugin.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/chalkboard/style.css">
  <script>
    function extend() {
      var target = {};
      for (var i = 0; i < arguments.length; i++) {
        var source = arguments[i];
        for (var key in source) {
          if (source.hasOwnProperty(key)) {
            target[key] = source[key];
          }
        }
      }
      return target;
    }

    // default options to init reveal.js
    var defaultOptions = {
      controls: true,
      progress: true,
      history: true,
      center: false,
      width: 1000,
      pdfSeparateFragments: false,
      transition: 'slide',
      transitionSpeed: 'fast',
      slideNumber: "c/t",
      highlight: {
        highlightOnLoad: false
      },
      plugins: [
        RevealMarkdown,
        RevealHighlight,
        RevealZoom,
        RevealNotes,
        RevealMath.KaTeX,
        RevealChalkboard,
        RevealCustomControls,
      ],
      customcontrols: {
        controls: [
          {
            icon: '<i class="fa fa-pen-square"></i>',
            title: 'Toggle chalkboard (B)',
            action: 'RevealChalkboard.toggleChalkboard();'
          },
          {
            icon: '<i class="fa fa-pen"></i>',
            title: 'Toggle notes canvas (C)',
            action: 'RevealChalkboard.toggleNotesCanvas();'
          }
        ]
      },
      chalkboard: {
        // add configuration here
      },
    };

    // options from URL query string
    var queryOptions = Reveal().getQueryHash() || {};

    var options = extend(defaultOptions, {"_":["./Lecture_10_NLP/Lecture_10_NLP.md"],"static":"static/Lecture_10_NLP","template":"./assets/reveal.html","preprocessor":"./assets/preproc.js","scripts":"assets/menu/menu.js,assets/inject.js"}, queryOptions);
  </script>

  <script src="./_assets/assets/menu/menu.js"></script>
  <script src="./_assets/assets/inject.js"></script>

  <script>
    Reveal.initialize(options);
    Reveal.addEventListener('ready', function (event) {
      const blocks = Reveal.getRevealElement().querySelectorAll('pre code:not(.mermaid)');
      const hlp = Reveal.getPlugin('highlight');
      blocks.forEach(hlp.highlightBlock);
    });
  </script>

  <script>
    const mermaidOptions = extend({ startOnLoad: false }, {});
    mermaid.startOnLoad = false;
    mermaid.initialize(mermaidOptions);
    const cb = function (event) {
      mermaid.init(mermaidOptions, '.stack.present > .present pre code.mermaid');
      mermaid.init(mermaidOptions, '.slides > .present:not(.stack) pre code.mermaid');
    }
    Reveal.addEventListener('ready', cb);
    Reveal.addEventListener('slidetransitionend', cb);
  </script>
</body>

</html>